{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvO9MT2Q3xQOFKq+NsEQqK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geekzenpro/material-dashboard/blob/master/sp_exo_local_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7uQtdfSs0JU"
      },
      "outputs": [],
      "source": [
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import requests\n",
        "import traceback\n",
        "from pandas.tseries.offsets import BDay\n",
        "import numpy as np\n",
        "from pycaret.datasets import get_data\n",
        "from pycaret.time_series import TSForecastingExperiment\n",
        "import os\n",
        "import pickle\n",
        "global_fig_settings = {\n",
        "    # \"renderer\": \"notebook\",\n",
        "    \"renderer\": \"png\",\n",
        "    \"width\": 1000,\n",
        "    \"height\": 600,\n",
        "}\n",
        "\n",
        "def save_columns(df, filename):\n",
        "    \"\"\"Save the columns of a DataFrame to a file\"\"\"\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(df.columns.tolist(), f)\n",
        "\n",
        "def load_columns(filename):\n",
        "    \"\"\"Load the columns of a DataFrame from a file\"\"\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "\n",
        "def calc_next_friday(d):\n",
        "    d = datetime.strptime(d, '%Y-%m-%d')\n",
        "    d += timedelta(7)\n",
        "    while d.weekday() != 4:\n",
        "        d += timedelta(1)\n",
        "    return d\n",
        "\n",
        "# Function to calculate the expected move using options data\n",
        "def calculate_expected_move_from_polygonio(symbol, exp_date, close_price):\n",
        "\n",
        "    api_key='JvOlZnErKIgvJ5yECAnUK6OFBoeKTioJ'\n",
        "\n",
        "    last_friday_price = close_price\n",
        "    last_friday = exp_date\n",
        "    #contractExpirydate = datetime.strptime(exp_date, '%Y-%m-%d').strftime('%y%m%d')\n",
        "    calculated_Next_Friday = calc_next_friday(exp_date)\n",
        "    print(\"calculated_Next_Friday: \", calculated_Next_Friday)\n",
        "    contractExpirydate =  calculated_Next_Friday.strftime('%y%m%d')\n",
        "    print(\"contractExpirydate: \", contractExpirydate)\n",
        "    #first_value = last_friday_price.iloc[0]\n",
        "    print(\"last_friday_price: \", last_friday_price)\n",
        "    if (last_friday_price < 100).all():\n",
        "        polygonCallContractName = \"O:\"+symbol+contractExpirydate+\"P\"+ \"000\"+str(int(last_friday_price))+ \"000\"\n",
        "    elif last_friday_price >= 1000 and last_friday_price < 10000:\n",
        "        polygonCallContractName = \"O:\"+symbol+contractExpirydate+\"P\"+ \"0\"+str(int(last_friday_price))+ \"000\"\n",
        "    else:\n",
        "        polygonCallContractName = \"O:\"+symbol+contractExpirydate+\"P\"+ \"00\"+str(int(last_friday_price))+ \"000\"\n",
        "\n",
        "    #frame polygon put contract name:\n",
        "    if last_friday_price < 100:\n",
        "        polygonPutContractName = \"O:\"+symbol+contractExpirydate+\"C\"+ \"000\"+str(int(last_friday_price))+ \"000\"\n",
        "    elif last_friday_price >= 1000 and last_friday_price < 10000:\n",
        "        polygonPutContractName = \"O:\"+symbol+contractExpirydate+\"C\"+ \"0\"+str(int(last_friday_price))+ \"000\"\n",
        "    else:\n",
        "        polygonPutContractName = \"O:\"+symbol+contractExpirydate+\"C\"+ \"00\"+str(int(last_friday_price))+ \"000\"\n",
        "\n",
        "    #print(polygonCallContractName)\n",
        "    #print(polygonPutContractName)\n",
        "    try:\n",
        "        #get the options call data from polygon.io\n",
        "        url = f'https://api.polygon.io/v2/aggs/ticker/{polygonCallContractName}/range/1/day/{last_friday}/{last_friday}?adjusted=true&sort=asc&limit=50000&apiKey={api_key}'\n",
        "        #print(url)\n",
        "        options_data = requests.get(url).json()\n",
        "        #print(options_data)\n",
        "        atm_call_close_price = options_data['results'][0]['c']\n",
        "        #print(polygonCallContractName, atm_call_close_price )\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to get options data for call: {e}\")\n",
        "        # print(\"Traceback: \\n\")\n",
        "        # traceback.print_exc()  # This will print detailed traceback\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        #get the options put data from polygon.io\n",
        "        url = f'https://api.polygon.io/v2/aggs/ticker/{polygonPutContractName}/range/1/day/{last_friday}/{last_friday}?adjusted=true&sort=asc&limit=50000&apiKey={api_key}'\n",
        "        #print(url)\n",
        "        options_data = requests.get(url).json()\n",
        "        #print(options_data)\n",
        "        atm_put_close_price = options_data['results'][0]['c']\n",
        "        #print(polygonPutContractName, atm_put_close_price)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to get options data for put: {e}\")\n",
        "        # print(\"Traceback: \\n\")\n",
        "        # traceback.print_exc()  # This will print detailed traceback\n",
        "        return None\n",
        "\n",
        "\n",
        "    try:\n",
        "\n",
        "        #calculate the expected move\n",
        "        #expected_move_friday = (atm_call_close_price + atm_put_close_price) * math.sqrt(7 / 365)  # 7 days to expiration\n",
        "        print(\"ATM Call Close Price: \", atm_call_close_price)\n",
        "        print(\"ATM Put Close Price: \", atm_put_close_price)\n",
        "\n",
        "        expected_move_friday = ((atm_call_close_price + atm_put_close_price)/(close_price))*100\n",
        "        print(\"Expected_Move_Friday\",expected_move_friday)\n",
        "\n",
        "        stock_price = last_friday_price\n",
        "\n",
        "        expected_high_friday = stock_price + expected_move_friday\n",
        "        expected_low_friday = stock_price - expected_move_friday\n",
        "\n",
        "        #return polygonCallContractName, polygonPutContractName, round(expected_low_friday, 2), round(expected_high_friday, 2), round(last_friday_price, 2), round(expected_move_friday, 2)\n",
        "        #time.sleep(2)\n",
        "        return round(expected_move_friday, 2)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to calculate expected move: {e}\")\n",
        "        return None\n",
        "\n",
        "def calculate_ATM_CP_Price_from_polygonio(symbol, exp_date, close_price):\n",
        "\n",
        "    api_key='JvOlZnErKIgvJ5yECAnUK6OFBoeKTioJ'\n",
        "\n",
        "    last_friday_price = close_price\n",
        "    last_friday = exp_date\n",
        "    #contractExpirydate = datetime.strptime(exp_date, '%Y-%m-%d').strftime('%y%m%d')\n",
        "    calculated_Next_Friday = calc_next_friday(exp_date)\n",
        "    print(\"calculated_Next_Friday: \", calculated_Next_Friday)\n",
        "    contractExpirydate =  calculated_Next_Friday.strftime('%y%m%d')\n",
        "    print(\"contractExpirydate: \", contractExpirydate)\n",
        "    #first_value = last_friday_price.iloc[0]\n",
        "    print(\"last_friday_price: \", last_friday_price)\n",
        "    if (last_friday_price < 100).all():\n",
        "        polygonCallContractName = \"O:\"+symbol+contractExpirydate+\"P\"+ \"000\"+str(int(last_friday_price))+ \"000\"\n",
        "    elif last_friday_price >= 1000 and last_friday_price < 10000:\n",
        "        polygonCallContractName = \"O:\"+symbol+contractExpirydate+\"P\"+ \"0\"+str(int(last_friday_price))+ \"000\"\n",
        "    else:\n",
        "        polygonCallContractName = \"O:\"+symbol+contractExpirydate+\"P\"+ \"00\"+str(int(last_friday_price))+ \"000\"\n",
        "\n",
        "    #frame polygon put contract name:\n",
        "    if last_friday_price < 100:\n",
        "        polygonPutContractName = \"O:\"+symbol+contractExpirydate+\"C\"+ \"000\"+str(int(last_friday_price))+ \"000\"\n",
        "    elif last_friday_price >= 1000 and last_friday_price < 10000:\n",
        "        polygonPutContractName = \"O:\"+symbol+contractExpirydate+\"C\"+ \"0\"+str(int(last_friday_price))+ \"000\"\n",
        "    else:\n",
        "        polygonPutContractName = \"O:\"+symbol+contractExpirydate+\"C\"+ \"00\"+str(int(last_friday_price))+ \"000\"\n",
        "\n",
        "    #print(polygonCallContractName)\n",
        "    #print(polygonPutContractName)\n",
        "    try:\n",
        "        #get the options call data from polygon.io\n",
        "        url = f'https://api.polygon.io/v2/aggs/ticker/{polygonCallContractName}/range/1/day/{last_friday}/{last_friday}?adjusted=true&sort=asc&limit=50000&apiKey={api_key}'\n",
        "        #print(url)\n",
        "        options_data = requests.get(url).json()\n",
        "        #print(options_data)\n",
        "        atm_call_close_price = options_data['results'][0]['c']\n",
        "        #print(polygonCallContractName, atm_call_close_price )\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to get options data for call: {e}\")\n",
        "        # print(\"Traceback: \\n\")\n",
        "        # traceback.print_exc()  # This will print detailed traceback\n",
        "        atm_call_close_price = 0\n",
        "\n",
        "    try:\n",
        "        #get the options put data from polygon.io\n",
        "        url = f'https://api.polygon.io/v2/aggs/ticker/{polygonPutContractName}/range/1/day/{last_friday}/{last_friday}?adjusted=true&sort=asc&limit=50000&apiKey={api_key}'\n",
        "        #print(url)\n",
        "        options_data = requests.get(url).json()\n",
        "        #print(options_data)\n",
        "        atm_put_close_price = options_data['results'][0]['c']\n",
        "        #print(polygonPutContractName, atm_put_close_price)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to get options data for put: {e}\")\n",
        "        # print(\"Traceback: \\n\")\n",
        "        # traceback.print_exc()  # This will print detailed traceback\n",
        "        atm_call_close_price = 0\n",
        "\n",
        "\n",
        "    try:\n",
        "\n",
        "        #calculate the expected move\n",
        "        #expected_move_friday = (atm_call_close_price + atm_put_close_price) * math.sqrt(7 / 365)  # 7 days to expiration\n",
        "        print(\"ATM Call Close Price: \", atm_call_close_price)\n",
        "        print(\"ATM Put Close Price: \", atm_put_close_price)\n",
        "\n",
        "\n",
        "        return round(atm_call_close_price,2), round(atm_put_close_price,2)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to calculate ATM PC Price: {e}\")\n",
        "        return 0, 0\n",
        "\n",
        "def calculate_returns_for_dates(pull_start_date, pull_end_date, tickers, EM_tickers):\n",
        "    returns = pd.DataFrame()\n",
        "    data = pd.DataFrame()\n",
        "    # Create an empty DataFrame for storing the returns\n",
        "    print(\"Pull Start Date: \", pull_start_date)\n",
        "    print(\"Pull End Date: \", pull_end_date)\n",
        "    # Download the historical data and calculate returns for each ticker\n",
        "    for ticker in tickers:\n",
        "        data = yf.download(ticker, start=pull_start_date, end=pull_end_date)\n",
        "    #   close = data['Adj Close']\n",
        "    #   open = data['Open']\n",
        "    #  #temp additions of open & close for comaprisions\n",
        "    #   ret = open.rename(ticker + \"_open\")\n",
        "    #   returns = pd.concat([returns, ret], axis=1)\n",
        "    #   ret = close.rename(ticker + \"_close\")\n",
        "    #   returns = pd.concat([returns, ret], axis=1)\n",
        "\n",
        "        #Quant Returns\n",
        "        close = data['Adj Close'].shift(-1)\n",
        "        open = data['Open']\n",
        "        ret = ((open - close) / close).fillna(0).rename(ticker +\"_qreturns\")\n",
        "        #ret = close.pct_change().rename(ticker)\n",
        "        # round of the returns to 4 decimal places\n",
        "        ret = round(ret*100, 2)\n",
        "        returns = pd.concat([returns, ret], axis=1)\n",
        "\n",
        "        close = data['Adj Close']\n",
        "        #open returns for the next day\n",
        "        open = data['Open']\n",
        "\n",
        "        ret = open.pct_change().shift(-1).rename(ticker +\"_oreturns\")\n",
        "        #ret = close.pct_change().rename(ticker)\n",
        "        # round of the returns to 4 decimal places\n",
        "        ret = round(ret*100, 2)\n",
        "        returns = pd.concat([returns, ret], axis=1)\n",
        "\n",
        "        #close returns for the next day\n",
        "        ret = close.pct_change().shift(-1).rename(ticker +\"_creturns\")\n",
        "        #ret = close.pct_change().rename(ticker)\n",
        "        # round of the returns to 4 decimal places\n",
        "        ret = round(ret*100, 2)\n",
        "        returns = pd.concat([returns, ret], axis=1)\n",
        "\n",
        "        #high returns for the next day\n",
        "        high = data['High']\n",
        "        ret = ((close - high) / close).rename(ticker +\"_hreturns\")\n",
        "        ret = round(ret*100, 2)\n",
        "        returns = pd.concat([returns, ret], axis=1)\n",
        "\n",
        "        #low returns for the next day\n",
        "        low = data['Low']\n",
        "        ret = ((close - low) / close).rename(ticker +\"_lreturns\")\n",
        "        ret = round(ret*100, 2)\n",
        "        returns = pd.concat([returns, ret], axis=1)\n",
        "\n",
        "        #capture daily open to close returns\n",
        "        ret = ((close - open) / open).rename(ticker + \"_ocreturns\")\n",
        "        ret = round(ret*100, 2)\n",
        "        returns = pd.concat([returns, ret], axis=1)\n",
        "\n",
        "        data[ticker + '_close'] = round(data['Close'],2)\n",
        "        returns = pd.concat([returns, data[ticker + '_close']], axis=1)\n",
        "\n",
        "        new_data = pd.DataFrame(index=returns.index)\n",
        "        if ticker in EM_tickers:\n",
        "            # Start with an empty dictionary\n",
        "            expected_moves = {}\n",
        "            polygonCount=0\n",
        "            EMCallDataIssue=False\n",
        "            # Iterate through each date in the returns DataFrame\n",
        "\n",
        "            for date in returns.index:\n",
        "                # Calculate the last Friday's date for the current date\n",
        "                last_friday = date - timedelta(days=(date.weekday() - 4) % 7)\n",
        "                #print(date, last_friday)\n",
        "                # Format the date as a string for the API call\n",
        "                last_friday_str = last_friday.strftime('%Y-%m-%d')\n",
        "                print(\"Price close Date:\", date, \"Last Friday Date\",last_friday_str)\n",
        "                # Check if the expected move for the last Friday already exists in the dictionary\n",
        "                #print(returns)\n",
        "                #print(returns.loc[date, ticker + '_close'])\n",
        "                if last_friday_str not in expected_moves:\n",
        "                    EMCallDataIssue=False\n",
        "                    # If it doesn't exist, calculate the expected move and store it in the dictionary\n",
        "                    #print(\"EM for this \", last_friday_str, \" Not Available\", ticker, returns.loc[date, ticker + '_close'])\n",
        "                    exected_move_as_of_friday = calculate_expected_move_from_polygonio(ticker, last_friday_str, returns.loc[date, ticker + '_close'])\n",
        "                    #time.sleep(31)\n",
        "                    polygonCount=polygonCount+1\n",
        "                    #print(polygonCount)\n",
        "                    if exected_move_as_of_friday is None:\n",
        "                    # Calculate the date of the previous Friday\n",
        "                        prev_last_friday = date - timedelta(days=(date.weekday() - 4) % 7 + 7)\n",
        "                        prev_last_friday_str = prev_last_friday.strftime('%Y-%m-%d')\n",
        "                        # Get the expected move from the previous Friday from the dictionary\n",
        "                        if prev_last_friday_str in expected_moves:\n",
        "                            #print(\"pulling data from previous last Friday, previous last Friday is \", prev_last_friday_str)\n",
        "                            # print(expected_moves)\n",
        "                            expected_moves[last_friday_str] = expected_moves[prev_last_friday_str]\n",
        "                            expected_move = expected_moves[prev_last_friday_str]\n",
        "                            EMCallDataIssue=True\n",
        "                        else:\n",
        "                            #print(\"Printing all Expected Moves calculated so far\")\n",
        "                            # print(expected_moves)\n",
        "                            # print(f\"No expected move available for date {prev_last_friday_str}\")\n",
        "                            #Assume a 2% expected move if no data is available previously as well\n",
        "                            expected_moves[last_friday_str] = returns.loc[date, ticker + '_close'] * 0.02\n",
        "                            expected_move = expected_moves[last_friday_str]\n",
        "                            EMCallDataIssue=True\n",
        "                    else:\n",
        "                        expected_moves[last_friday_str] = exected_move_as_of_friday\n",
        "                        expected_move = exected_move_as_of_friday\n",
        "                        EMCallDataIssue=False\n",
        "                        #print(\"No Calculation Issue for \", last_friday_str, \" Expected Move is \", expected_move)\n",
        "\n",
        "                # Retrieve the expected move for last Friday\n",
        "                print(\"Expected Move\", ticker, last_friday_str, expected_moves[last_friday_str])\n",
        "                expected_move = expected_moves[last_friday_str]\n",
        "                # Calculate expected_move_percent\n",
        "                #expected_move_percent = expected_move / returns.loc[date, ticker + '_close']\n",
        "                #check if returns.loc[last_friday, ticker + '_close']  exists or not\n",
        "\n",
        "                if last_friday in returns.index:\n",
        "                    expected_move_percent =  (returns.loc[date, ticker + '_close'] - returns.loc[last_friday, ticker + '_close'] )/expected_move\n",
        "                    # print (\"Last Friday Close\", returns.loc[last_friday, ticker + '_close'])\n",
        "                    # print (\"Today Close\", returns.loc[date, ticker + '_close'])\n",
        "                    # print (\"Expected Move \", expected_move)\n",
        "                    # print (\"Expected Move Percent\", expected_move_percent)\n",
        "                    lower_bound = returns.loc[last_friday, ticker + '_close'] - expected_move\n",
        "                    upper_bound = returns.loc[last_friday, ticker + '_close'] + expected_move\n",
        "                    if returns.loc[date, ticker + '_close'] < lower_bound:\n",
        "                        expected_move_range = -1\n",
        "                    elif returns.loc[date, ticker + '_close'] > upper_bound:\n",
        "                        expected_move_range = 1\n",
        "                    else:\n",
        "                        expected_move_range = 0\n",
        "                else:\n",
        "                    expected_move_percent =  0\n",
        "                    expected_move_range = 0\n",
        "                # Calculate days_to_expire\n",
        "                days_to_expire = len(pd.bdate_range(date, last_friday + pd.DateOffset(weeks=1))) - 1\n",
        "                #calculate the ATM Call & Put Price for the day\n",
        "                atm_call_price, atm_put_price = calculate_ATM_CP_Price_from_polygonio(ticker, last_friday_str, returns.loc[date, ticker + '_close'])\n",
        "                # if EMCallDataIssue is True:\n",
        "                #      new_data.loc[date, ticker + '_EMCalcIssue'] = \"True - PreviousWeekEMisUsed\"\n",
        "                # else:\n",
        "                #      new_data.loc[date, ticker + '_EMCalcIssue'] = \"False - No Issue\"\n",
        "                new_data.loc[date, ticker + '_EM'] = expected_move\n",
        "                new_data.loc[date, ticker + '_AtmCallPrice'] = atm_call_price\n",
        "                new_data.loc[date, ticker + '_AtmPutPrice'] = atm_put_price\n",
        "                new_data.loc[date, ticker + '_Pct_of_EM'] = round(expected_move_percent,4)\n",
        "                new_data.loc[date, ticker + '_EM_DTE'] = days_to_expire\n",
        "                new_data.loc[date, ticker + '_EM_BR'] = expected_move_range\n",
        "                # Concatenate the two DataFrames along axis=1\n",
        "            returns = pd.concat([returns, new_data], axis=1)\n",
        "    return returns\n",
        "\n",
        "def ExtractAndPrepData(start_date, end_date, tickers,EM_tickers, cached_returns_file, target, tradeday):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to Extract and Prepare Data.\n",
        "    \"\"\"\n",
        "    start_date = pd.to_datetime(start_date)\n",
        "    end_date = pd.to_datetime(end_date)\n",
        "    tradeday = pd.to_datetime(tradeday)\n",
        "\n",
        "    root_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "    exo_data_file_path = os.path.join(root_dir, 'exo-data-files', cached_returns_file)\n",
        "    cached_returns_file_w_path = os.path.join(root_dir, exo_data_file_path)\n",
        "\n",
        "    root_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "    model_name = \"exo_model_target-\" + target + str(forcast_days_fh)\n",
        "    modelColumnsFileName = model_name + \"_columns.pkl\"\n",
        "    modelColumnsFileName_with_path = os.path.join(root_dir, 'exo-model-attributes', modelColumnsFileName)\n",
        "\n",
        "    print(cached_returns_file_w_path)\n",
        "    #cached_returns_file_w_path = '/Users/zenpro/src/mqc7/lchatgpt/quantgalore/exoquant/AMZN_ARIMAX_cached_returns_data_file.csv'\n",
        "    #print(os.path)\n",
        "    returnsAppeneded = False\n",
        "    #returns = pd.read_csv(cached_returns_file, index_col=0)\n",
        "    if os.path.exists(cached_returns_file_w_path):\n",
        "        returns = pd.read_csv(cached_returns_file_w_path)\n",
        "        if 'Unnamed: 0' in returns.columns:\n",
        "            returns = returns.drop(columns=['Unnamed: 0'])\n",
        "        returns.set_index('Date', inplace=True)\n",
        "        returns.index = pd.to_datetime(returns.index)\n",
        "        print(returns)\n",
        "        returns = returns[~returns.index.duplicated(keep='first')]\n",
        "        print(\"deduplicate\", returns)\n",
        "        #returns['Date'] = returns.index.date\n",
        "        print(returns.head())\n",
        "    else:\n",
        "        returns = pd.DataFrame()\n",
        "    #print(returns.head())\n",
        "    if not returns.empty:\n",
        "        min_cached_date = returns.index.min()\n",
        "        max_cached_date = returns.index.max()\n",
        "        print(\"cache min & max \", min_cached_date, max_cached_date)\n",
        "        print(\"start & end date \",start_date, end_date)\n",
        "        if max_cached_date < end_date:\n",
        "            print(\"inside less\")\n",
        "\n",
        "            print(\"max_cached_date\", max_cached_date)\n",
        "            print(\"end_date\", end_date)\n",
        "\n",
        "            missing_returns_end =   calculate_returns_for_dates(max_cached_date , end_date+ pd.Timedelta(days=1), tickers, EM_tickers)\n",
        "            print(missing_returns_end)\n",
        "            returns = pd.concat([returns, missing_returns_end])\n",
        "            returnsAppeneded = True\n",
        "\n",
        "        if min_cached_date > start_date:\n",
        "            print(\"greater than\")\n",
        "            print(\"min_cached_date\", min_cached_date)\n",
        "            print(\"start_date\", start_date)\n",
        "            missing_returns_start =  calculate_returns_for_dates(start_date- pd.Timedelta(days=1), min_cached_date , tickers, EM_tickers)\n",
        "            print(missing_returns_start)\n",
        "            returns = pd.concat([missing_returns_start, returns])\n",
        "            returnsAppeneded = True\n",
        "    else:\n",
        "        print(\"returns file is empty\")\n",
        "        returns = calculate_returns_for_dates(start_date,end_date, tickers, EM_tickers)\n",
        "        returns_sc = returns.copy()\n",
        "        print(returns_sc.head())\n",
        "        returns_sc.index = pd.to_datetime(returns_sc.index) # make sure the index is a DatetimeIndex\n",
        "        returns_sc['Date'] = returns_sc.index.date\n",
        "        returns_sc = returns_sc[returns_sc.index < tradeday]\n",
        "        returns_sc.reset_index(drop=True, inplace=True)\n",
        "        returns_sc = returns_sc.loc[~returns_sc.index.duplicated(keep='first')]\n",
        "        returns_sc.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "        returns_sc.fillna(0, inplace=True)\n",
        "        # returns_sc = returns_sc.iloc[:-1]\n",
        "        returns_sc.to_csv(cached_returns_file_w_path)\n",
        "        print(\"New returns file is created\")\n",
        "    #returns.sort_index(inplace=True)\n",
        "    if (returnsAppeneded == True):\n",
        "        returns_sca = returns.copy()\n",
        "        returns_sca.index = pd.to_datetime(returns_sca.index) # make sure the index is a DatetimeIndex\n",
        "        returns_sca['Date'] = returns_sca.index.date\n",
        "        returns_sca = returns_sca[returns_sca.index < tradeday]\n",
        "        returns_sca.reset_index(drop=True, inplace=True)\n",
        "        returns_sca = returns_sca.loc[~returns_sca.index.duplicated(keep='first')]\n",
        "        returns_sca.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "        returns_sca.fillna(0, inplace=True)\n",
        "        returns_sca.to_csv(cached_returns_file_w_path)\n",
        "    # restrict to desired date range\n",
        "    returns = returns.loc[start_date:end_date]\n",
        "    returns = returns[~returns.index.duplicated(keep='first')]\n",
        "    #return returns\n",
        "    # #today = datetime.today()+ timedelta(days=1)\n",
        "    # today = datetime.today()+ timedelta(days=-2)\n",
        "    # predict_end_date =today.strftime('%Y-%m-%d')\n",
        "    # #start date is 180 trading days before the end date\n",
        "    # #predict_start_date = datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=2)\n",
        "    # predict_start_date = datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=5)\n",
        "    # print(\"predict_start_date\", predict_start_date)\n",
        "    # print(\"predict_end_date\", predict_end_date)\n",
        "    # Ensure the index is a DatetimeIndex\n",
        "    returns.index = pd.to_datetime(returns.index)\n",
        "\n",
        "    # Create a mask for dates prior to today\n",
        "    mask = returns.index.date < tradeday.date()\n",
        "\n",
        "    # Apply the mask to the DataFrame\n",
        "    returns = returns.loc[mask]\n",
        "\n",
        "    # Add a 'Date' column based on the index\n",
        "    returns['Date'] = returns.index.date\n",
        "\n",
        "    # Reset the index\n",
        "    returns.reset_index(drop=True, inplace=True)\n",
        "    returns[\"index\"] = pd.to_datetime(returns[\"Date\"])\n",
        "    returns.drop(columns=[\"Date\"], inplace=True)\n",
        "    #returns.replace(-200, np.nan, inplace=True)\n",
        "    #save the returns file only if there is any changes to the file\n",
        "\n",
        "\n",
        "    #use the tickers to create the list of new fields  that ends with _close\n",
        "    exclude = [ticker + '_close' for ticker in tickers]\n",
        "    #print(exclude)\n",
        "    data_exo = returns.copy()\n",
        "    data_exo.drop(columns=exclude, inplace=True)\n",
        "    data_exo.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "    data_exo = data_exo.loc[~data_exo.index.duplicated(keep='first')]\n",
        "    print(\"printing duplicate data_exo index\")\n",
        "    print(data_exo.index.duplicated().sum())\n",
        "\n",
        "    #data_exo = data_exo.loc[~returns.index.duplicated(keep='first')]\n",
        "    # Remove rows with all NaN values\n",
        "    data_exo.dropna(how='all', inplace=True)\n",
        "\n",
        "    # If there are still NaN values, you can fill them with an appropriate value, such as 0\n",
        "    data_exo.fillna(0, inplace=True)\n",
        "\n",
        "    # Make sure there are no duplicates\n",
        "    assert data_exo.index.duplicated().sum() == 0\n",
        "    data_exo.set_index(\"index\", inplace=True)\n",
        "    data_exo = data_exo.asfreq('B')\n",
        "    data_exo.head()\n",
        "\n",
        "    save_columns(data_exo, modelColumnsFileName_with_path)\n",
        "    #print(type(data_exo))\n",
        "    #print(data_exo.index)\n",
        "    #data_exo.to_csv(target+\"exo_data.csv\")\n",
        "    return data_exo\n",
        "\n",
        "def createFinalizeSaveModel(data_input, target, forcast_days_fh):\n",
        "    exp_exo = TSForecastingExperiment()\n",
        "    #print(data_input)\n",
        "    print(type(data_input))\n",
        "    #print(data_input.index)\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    #print(data_input)\n",
        "    print(\"length of exog_var: \", len(data_input.columns))\n",
        "    print(\"length of exog_var: \", len(data_input))\n",
        "    max_day_trained_data = data_input.index.max()\n",
        "    print(\"max_day_trained_data\", max_day_trained_data)\n",
        "    exp_exo.setup(\n",
        "        data=data_input, target=target, fh=forcast_days_fh,\n",
        "        numeric_imputation_target=\"ffill\", numeric_imputation_exogenous=\"ffill\",\n",
        "        fig_kwargs=global_fig_settings, session_id=12\n",
        "    )\n",
        "    model_exo = exp_exo.create_model(\"auto_arima\")\n",
        "    # data_exo.plot()\n",
        "    # exp_exo.plot_model(model_exo)\n",
        "    #tuned_model_exo = exp.tune_model(model_exo)\n",
        "\n",
        "    final_exo_model = exp_exo.finalize_model(model_exo)\n",
        "    root_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "    model_name = \"exo_model_target-\" + target + str(forcast_days_fh)\n",
        "    models_FilePath = os.path.join(root_dir, 'exo-models', model_name)\n",
        "    _ = exp_exo.save_model(final_exo_model, models_FilePath)\n",
        "\n",
        "def prediction(target, tickers, EM_tickers, predict_start_date, predict_end_date, forcast_days_fh):\n",
        "    td_returns = calculate_returns_for_dates(predict_start_date,predict_end_date, tickers, EM_tickers)\n",
        "    # Ensure the index is a DatetimeIndex\n",
        "    td_returns.index = pd.to_datetime(td_returns.index)\n",
        "    # # Create a mask for dates prior to today\n",
        "    # mask = returns.index.date >= datetime.today().date()\n",
        "    # # Apply the mask to the DataFrame\n",
        "    # returns = returns.loc[mask]\n",
        "\n",
        "    # Add a 'Date' column based on the index\n",
        "    exclude = [target]\n",
        "    td_returns.drop(columns=exclude, inplace=True)\n",
        "\n",
        "    td_returns['Date'] = td_returns.index.date\n",
        "\n",
        "    # Reset the index\n",
        "    td_returns.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    td_returns[\"index\"] = pd.to_datetime(td_returns[\"Date\"])\n",
        "    td_returns.drop(columns=[\"Date\"], inplace=True)\n",
        "    #td_returns.head()\n",
        "    #get close price from the last day of the tdReturns dataframe\n",
        "    #print(td_returns.tail(1))\n",
        "    #print(td_returns.tail(1).index)\n",
        "    td_max_date = td_returns.tail(1).index\n",
        "    td_close_price = td_returns.tail(1)['SPY_close']\n",
        "    print(\"td_max_date\", td_max_date)\n",
        "    print(\"td_close_price\", td_close_price)\n",
        "    print(td_returns)\n",
        "\n",
        "    td_returns.iloc[1] = td_returns.iloc[1].fillna(td_returns.iloc[0])\n",
        "    # Reset the index\n",
        "    ftd_returns = td_returns.drop(td_returns.index[0])\n",
        "    # exclude = ['XLK_close', 'XLF_close','XLI_close', 'XLE_close' ,'XLY_close','XLP_close','XLU_close','XLV_close','XLB_close', 'XLRE_close','XLC_close','SPY_close','QQQ_close','VXX_close', 'UVXY_close','^VIX_close', '^VIX9D_close', '^VIX3M_close']\n",
        "\n",
        "    # ftd_returns.drop(columns=exclude, inplace=True)\n",
        "    ftd_returns.set_index(\"index\", inplace=True)\n",
        "    #data_exo.reindex(schedule.index)\n",
        "    ftd_returns = ftd_returns.asfreq('B')\n",
        "    ftd_returns.index = ftd_returns.index.to_period('B')\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    #print(ftd_returns)\n",
        "    exp_future = TSForecastingExperiment()\n",
        "\n",
        "    model_name = \"exo_model_target-\" + target + str(forcast_days_fh)\n",
        "    root_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "    modelColumnFileName = model_name + \"_columns.pkl\"\n",
        "    modelColumnsFileName_with_path = os.path.join(root_dir, 'exo-model-attributes', modelColumnFileName)\n",
        "\n",
        "    root_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "    model_name = \"exo_model_target-\" + target + str(forcast_days_fh)\n",
        "    model_FilePath = os.path.join(root_dir, 'exo-models', model_name)\n",
        "    modelColumnsFileName = model_name + \"_columns.pkl\"\n",
        "    modelColumnsFileName_with_path = os.path.join(root_dir, 'exo-model-attributes', modelColumnsFileName)\n",
        "\n",
        "\n",
        "    final_exo_model = exp_future.load_model(model_FilePath)\n",
        "    columns_used_in_training = load_columns(modelColumnsFileName_with_path)\n",
        "\n",
        "    #print(\"columns_used_in_training\", columns_used_in_training)\n",
        "    print(\"length of columns_used_in_training\", len(columns_used_in_training))\n",
        "    columns_used_in_training.remove(target)\n",
        "    ftd_returns_exog = ftd_returns[columns_used_in_training]\n",
        "    ftd_returns_exog.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    ftd_returns_exog.fillna(0, inplace=True)\n",
        "    #print(ftd_returns_exog)\n",
        "    ftd_returns_exog_copy = ftd_returns_exog.copy()\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    print(\"length of exog_var: \", len(ftd_returns_exog.columns))\n",
        "    #print(\"index date in exog_var dataframe: \", ftd_returns_exog.index)\n",
        "    #print(\"exog_var:\\n\", ftd_returns_exog_copy)\n",
        "    future_preds = exp_future.predict_model(final_exo_model, X=ftd_returns_exog)\n",
        "    return future_preds, td_close_price, td_max_date\n",
        "\n",
        "#end date is the current date\n",
        "# today = datetime.today()+ timedelta(days=-1)\n",
        "# end_date =today.strftime('%Y-%m-%d')\n",
        "# #start date is 180 trading days before the end date\n",
        "# start_date = datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=29)\n",
        "# print(\"start_date: \", start_date)\n",
        "# print(\"end_date: \", end_date)\n",
        "\n",
        "#predict_start_date = datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=5)\n",
        "ticker = 'SPY'\n",
        "forcast_days_fh = 1\n",
        "EM_tickers = [ticker,'QQQ']\n",
        "target = ticker + \"_ocreturns\"\n",
        "tradeday = datetime.today().date()\n",
        "\n",
        "# If yesterday was a weekend, adjust 'today' to be the last trading day\n",
        "if tradeday.weekday() > 4:  # 0-4 corresponds to Monday-Friday\n",
        "    # Subtract the number of days to the last trading day (Friday)\n",
        "    tradeday = tradeday - timedelta(days=tradeday.weekday() - 4)\n",
        "\n",
        "# Convert 'today' back to a datetime object with time (at midnight)\n",
        "tradeday = pd.to_datetime(tradeday) #- timedelta(days=1)\n",
        "print(\"tradeday: \", tradeday)\n",
        "#tradeday = '2023-07-14'\n",
        "\n",
        "end_date = (tradeday+ timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "start_date = datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=-290)\n",
        "start_date = start_date.strftime('%Y-%m-%d')\n",
        "print(\"start_date: \", start_date)\n",
        "print(\"end_date: \", end_date)\n",
        "predict_end_date =(tradeday + timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "predict_start_date = datetime.strptime(predict_end_date, '%Y-%m-%d') - timedelta(days=2)\n",
        "predict_start_date = predict_start_date.strftime('%Y-%m-%d')\n",
        "tickers = [ticker, 'XLK', 'XLF','XLI', 'XLE' ,'XLY','XLP','XLU','XLV','XLB', 'XLRE','XLC','QQQ','VXX', 'UVXY','^VIX', '^VIX9D', '^VIX3M']\n",
        "cache_file_name = ticker + '_ARIMAX_cached_returns_data_file_enhanced.csv'\n",
        "data_exo = ExtractAndPrepData(start_date, end_date, tickers, EM_tickers, cache_file_name, target, tradeday)\n",
        "createFinalizeSaveModel(data_exo, target, forcast_days_fh)\n",
        "print(\"predict_start_date\", predict_start_date)\n",
        "print(\"predict_end_date\", predict_end_date)\n",
        "predResults , td_close_price, td_max_date= prediction(target, tickers, EM_tickers, predict_start_date, predict_end_date, forcast_days_fh)\n",
        "# get date and y_pred from predResults\n",
        "print(\"predResults: \", predResults)\n",
        "pd_date = predResults.index\n",
        "pd_Y_pred = predResults['y_pred']\n",
        "print(\"current price for \" + ticker +\":\", td_close_price)\n",
        "#get current price of SPX close price from yahoo finance\n",
        "\n",
        "if target == ticker + \"_ocreturns\":\n",
        "    spy_close_price = round(yf.download(ticker, start=tradeday, end=tradeday + pd.Timedelta(days=1))['Open'][0],2)\n",
        "\n",
        "    print(spy_close_price)\n",
        "    #print( target, + \" for \" + ticker + \" on \" + str(td_max_date) + \" is \" + str(td_close_price * (1 + pd_Y_pred)))\n",
        "    print(f\"{target} for {ticker} on {str(td_max_date)} is {str(spy_close_price * (1 + (pd_Y_pred/100)))}\")\n",
        "\n",
        "\n",
        "    spx_close_price = round(yf.download(\"^SPX\", start=tradeday, end=tradeday + pd.Timedelta(days=1))['Open'][0],2)\n",
        "\n",
        "    print(\"current price: \", td_close_price)\n",
        "    print(f\"{target} for SPX on {str(td_max_date)} is {str(spx_close_price * (1 + (pd_Y_pred/100)))}\")\n",
        "\n",
        "\n",
        "if target == ticker + \"_qreturns\":\n",
        "    spy_close_price = round(yf.download(ticker, start=tradeday, end=tradeday + pd.Timedelta(days=1))['Close'][0],2)\n",
        "\n",
        "    print(spy_close_price)\n",
        "    #print( target, + \" for \" + ticker + \" on \" + str(td_max_date) + \" is \" + str(td_close_price * (1 + pd_Y_pred)))\n",
        "    print(f\"{target} for {ticker} on {str(td_max_date)} is {str(spy_close_price * (1 + (pd_Y_pred/100)))}\")\n",
        "\n",
        "    spx_close_price = round(yf.download(\"^SPX\", start=tradeday, end=tradeday + pd.Timedelta(days=1))['Close'][0],2)\n",
        "\n",
        "    print(\"current price: \", td_close_price)\n",
        "    print(f\"{target} for SPX on {str(td_max_date)} is {str(spx_close_price * (1 + (pd_Y_pred/100)))}\")\n",
        "\n",
        "\n",
        "# spx_close_price = round(yf.download(\"^SPX\", start=tradeday, end=tradeday + pd.Timedelta(days=1))['Close'][0],2)\n",
        "\n",
        "# print(\"current price: \", td_close_price)\n",
        "# print(f\"{target} for SPX on {str(td_max_date)} is {str(spx_close_price * (1 + (pd_Y_pred/100)))}\")\n"
      ]
    }
  ]
}